{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets, Audio\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ðŸ¤— Datasets, downloading and preparing data is extremely simple. \n",
    "We can download and prepare the Common Voice splits in just one line of code. \n",
    "\n",
    "First, ensure you have accepted the terms of use on the Hugging Face Hub: [mozilla-foundation/common_voice_11_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). Once you have accepted the terms, you will have full access to the dataset and be able to download the data locally.\n",
    "\n",
    "Since Hindi is very low-resource, we'll combine the `train` and `validation` \n",
    "splits to give approximately 8 hours of training data. We'll use the 4 hours \n",
    "of `test` data as our held-out test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 12360\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 5069\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"sv-SE\", split=\"train+validation\", token=\"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\")\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"sv-SE\", split=\"test\", token=\"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\")\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 12360\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 5069\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since \n",
    "our input audio is sampled at 48kHz, we need to _downsample_ it to \n",
    "16kHz prior to passing it to the Whisper feature extractor, 16kHz being the sampling rate expected by the Whisper model. \n",
    "\n",
    "We'll set the audio inputs to the correct sampling rate using dataset's \n",
    "[`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column)\n",
    "method. This operation does not change the audio in-place, \n",
    "but rather signals to `datasets` to resample audio samples _on the fly_ the \n",
    "first time that they are loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the audio to 16kHz\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'num_samples', 'path', 'audio', 'transcription', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'],\n",
      "        num_rows: 2715\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'num_samples', 'path', 'audio', 'transcription', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'],\n",
      "        num_rows: 759\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "fleurs = DatasetDict()\n",
    "\n",
    "fleurs[\"train\"] = load_dataset(\"google/fleurs\", \"sv_se\", split=\"train+validation\", token=\"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\")\n",
    "fleurs[\"test\"] = load_dataset(\"google/fleurs\", \"sv_se\", split=\"test\", token=\"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\")\n",
    "\n",
    "print(fleurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 2715\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 759\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "fleurs = fleurs.remove_columns(['id', 'num_samples', 'path', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'])\n",
    "fleurs = fleurs.rename_column(\"transcription\", \"sentence\")\n",
    "\n",
    "print(fleurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the audio to 16kHz (Technically not needed since the audio is already 16kHz)\n",
    "fleurs = fleurs.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the datasets\n",
    "combined_dataset = DatasetDict()\n",
    "combined_dataset[\"train\"] = concatenate_datasets([common_voice[\"train\"], fleurs[\"train\"]])\n",
    "combined_dataset[\"test\"] = concatenate_datasets([common_voice[\"test\"], fleurs[\"test\"]])\n",
    "\n",
    "# Remove the old datasets to save memory\n",
    "del common_voice\n",
    "del fleurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load WhisperFeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Whisper feature extractor performs two operations:\n",
    "1. Pads / truncates the audio inputs to 30s: any audio inputs shorter than 30s are padded to 30s with silence (zeros), and those longer that 30s are truncated to 30s\n",
    "2. Converts the audio inputs to _log-Mel spectrogram_ input features, a visual representation of the audio and the form of the input expected by the Whisper model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/spectrogram.jpg\" alt=\"Trulli\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"><b>Figure 2:</b> Conversion of sampled audio array to log-Mel spectrogram.\n",
    "Left: sampled 1-dimensional audio signal. Right: corresponding log-Mel spectrogram. Figure source:\n",
    "<a href=\"https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html\">Google SpecAugment Blog</a>.\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the feature extractor from the pre-trained checkpoint with the default values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load WhisperTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Whisper model outputs a sequence of _token ids_. The tokenizer maps each of these token ids to their corresponding text string. For Hindi, we can load the pre-trained tokenizer and use it for fine-tuning without any further modifications. We simply have to \n",
    "specify the target language and the task. These arguments inform the \n",
    "tokenizer to prefix the language and task tokens to the start of encoded \n",
    "label sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"Swedish\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first example of the Common Voice dataset to see \n",
    "what form the data is in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/home/ubuntu/.cache/huggingface/datasets/downloads/extracted/0019eb248d80405b0e7e3f13e532a9c65fa377f1140361dec1d083cdead3aac2/sv-SE_train_0/common_voice_sv-SE_20466896.mp3', 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, 'sentence': 'Du ser ut att ha gjort det hÃ¤r hela livet.'}\n"
     ]
    }
   ],
   "source": [
    "print(combined_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a function to prepare our data ready for the model:\n",
    "1. We load and resample the audio data by calling `batch[\"audio\"]`. As explained above, ðŸ¤— Datasets performs any necessary resampling operations on the fly.\n",
    "2. We use the feature extractor to compute the log-Mel spectrogram input features from our 1-dimensional audio array.\n",
    "3. We encode the transcriptions to label ids through the use of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_common_voice_11_0(batch): \n",
    "    \"\"\"Function to preprocess the dataset with the .map method\"\"\"\n",
    "    # Prepare dataset provided by Mozilla Common Voice 11.0\n",
    "    # source: https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0#data-preprocessing-recommended-by-hugging-face\n",
    "    transcription = batch[\"sentence\"]\n",
    "\n",
    "    if transcription.startswith('\"') and transcription.endswith('\"'):\n",
    "        # we can remove trailing quotation marks as they do not affect the transcription\n",
    "        transcription = transcription[1:-1]\n",
    "\n",
    "    if transcription[-1] not in [\".\", \"?\", \"!\"]:\n",
    "        # append a full-stop to sentences that do not end in punctuation\n",
    "        transcription = transcription + \".\"\n",
    "\n",
    "    batch[\"sentence\"] = transcription\n",
    "\n",
    "    return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "def prepare_combined_dataset(batch):\n",
    "    # Chain the two prepare functions\n",
    "    return prepare_dataset(prepare_dataset_common_voice_11_0(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the data preparation function to all of our training examples using dataset's `.map` method. The argument `num_proc` specifies how many CPU cores to use. Setting `num_proc` > 1 will enable multiprocessing. If the `.map` method hangs with multiprocessing, set `num_proc=1` and process the dataset sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2385f3042f8445290020c0d310e9f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/15075 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5895e3efd64f67a476ef25aee56f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/5828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)\n",
    "combined_dataset = combined_dataset.map(prepare_combined_dataset, remove_columns=combined_dataset.column_names[\"train\"], num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1d9271b7124578ad0209ab6574fffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/29 shards):   0%|          | 0/15075 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb22c04aa854a55969b94cc40c3c9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/12 shards):   0%|          | 0/5828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_dataset.save_to_disk(\"combined_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/BlondeFer/Documents/Master/2_2/ID2223/Assignment_2\n",
      "['dataset_dict.json', 'test', 'train']\n",
      "['data-00000-of-00029.arrow', 'data-00001-of-00029.arrow', 'data-00002-of-00029.arrow', 'data-00003-of-00029.arrow', 'data-00004-of-00029.arrow', 'data-00005-of-00029.arrow', 'data-00006-of-00029.arrow', 'data-00007-of-00029.arrow', 'data-00008-of-00029.arrow', 'data-00009-of-00029.arrow', 'data-00010-of-00029.arrow', 'data-00011-of-00029.arrow', 'data-00012-of-00029.arrow', 'data-00013-of-00029.arrow', 'data-00014-of-00029.arrow', 'data-00015-of-00029.arrow', 'data-00016-of-00029.arrow', 'data-00017-of-00029.arrow', 'data-00018-of-00029.arrow', 'data-00019-of-00029.arrow', 'data-00020-of-00029.arrow', 'data-00021-of-00029.arrow', 'data-00022-of-00029.arrow', 'data-00023-of-00029.arrow', 'data-00024-of-00029.arrow', 'data-00025-of-00029.arrow', 'data-00026-of-00029.arrow', 'data-00027-of-00029.arrow', 'data-00028-of-00029.arrow', 'dataset_info.json', 'state.json']\n",
      "['data-00000-of-00012.arrow', 'data-00001-of-00012.arrow', 'data-00002-of-00012.arrow', 'data-00003-of-00012.arrow', 'data-00004-of-00012.arrow', 'data-00005-of-00012.arrow', 'data-00006-of-00012.arrow', 'data-00007-of-00012.arrow', 'data-00008-of-00012.arrow', 'data-00009-of-00012.arrow', 'data-00010-of-00012.arrow', 'data-00011-of-00012.arrow', 'dataset_info.json', 'state.json']\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "print(os.listdir(\"./combined_dataset/\"))\n",
    "print(os.listdir(\"./combined_dataset/train\"))\n",
    "print(os.listdir(\"./combined_dataset/test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.7GiB\n"
     ]
    }
   ],
   "source": [
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "            elif entry.is_dir():\n",
    "                total += get_dir_size(entry.path)\n",
    "    return total\n",
    "\n",
    "def sizeof_fmt(num, suffix=\"B\"):\n",
    "    # Source: https://web.archive.org/web/20111010015624/http://blogmag.net/blog/read/38/Print_human_readable_file_size\n",
    "    for unit in (\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\"):\n",
    "        if abs(num) < 1024.0:\n",
    "            return f\"{num:3.1f}{unit}{suffix}\"\n",
    "        num /= 1024.0\n",
    "    # If we get here, the size is too large to be represented as a PiB value\n",
    "    return f\"{num:.1f}Pi{suffix}\"\n",
    "\n",
    "\n",
    "sz = get_dir_size(\"./combined_dataset/\")\n",
    "print(sizeof_fmt(sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
